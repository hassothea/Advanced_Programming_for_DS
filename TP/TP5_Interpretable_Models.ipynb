{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6d2cf1",
   "metadata": {},
   "source": [
    "# **TP5 - Interpretable Models**\n",
    "\n",
    "**Course: Advanced Programming For Data Science** <br>\n",
    "**Lecturer: Dr. Sothea HAS** <br> \n",
    "**Mr. Vesal KHEAN**\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e2978",
   "metadata": {},
   "source": [
    "**Objective:** In this lab, you will move beyond simple prediction to understanding the *relationships* between physical measurements and the age of an abalone. You will learn to use statistical tests to select significant features, handle *non-linear patterns* with *polynomial features*, and control model complexity using *regularization*. The resulting models should also be interpreted at the end of this work.\n",
    "\n",
    "**The Jupyter Notebook for this TP can be downloaded here: [TP4_interpretable_Models.ipynb.](https://hassothea.github.io/Advanced_Programming_for_DS/TP/TP5_Interpretable_Models.ipynb){target='_blank'}****\n",
    "\n",
    "\n",
    "## **1\\. Abalone dataset: EDA and Test**\n",
    "\n",
    "[![](https://asc-aqua.org/wp-content/uploads/2023/03/shutterstock_1916916224-1-2500x1250.jpg)](https://asc-aqua.org/wp-content/uploads/2023/03/shutterstock_1916916224-1-2500x1250.jpg){target=\"_blank\"}\n",
    "\n",
    "Before interpreting any model, we must ensure the relationships it captures are statistically significant. We will use `statsmodels` to obtain detailed statistical summaries (`p-values`) for our coefficients.\n",
    "\n",
    "**A. Import dataset:** For more information about the dataset at [Abalone of UC Irvive Machine Learning Repository](https://archive.ics.uci.edu/dataset/1/abalone){target='_blank'}. The Abalone dataset can be imported from respository of Machine Learning as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aeb7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "column_names = [\"Type\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \n",
    "                \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
    "\n",
    "df = pd.read_csv(url, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dae553",
   "metadata": {},
   "source": [
    "**B. EDA and Data Preprocessing:** \n",
    "\n",
    "- Whatâ€™s the dimension of this dataset? How many quantitative and qualitative variables are there in this dataset?\n",
    "\n",
    "- Create statistical summary and visualize the distribution of the dataset. \n",
    "\n",
    "- Identify and handle problems if there are any in this dataset.\n",
    "\n",
    "- Study the correlation matrix of this dataset. Comment this correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c54a6",
   "metadata": {},
   "source": [
    "\n",
    "**C. Simple and Multiple Linear Regression (OLS)**\n",
    "\n",
    "- Split the dataset into $80\\%-20\\%$ train-test data.\n",
    "\n",
    "- Fit simple linear regression model (`slr`) predicting `Rings` using the most reasonable input and `statsmodels` module, here because it provides a comprehensive summary table for statistical testing.\n",
    "\n",
    "- Fit multiple linear regression model (`mlr`) using all available features. \n",
    "\n",
    "- Compute the performances of your models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc1721",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**D. Interpretation & Significance Testing:** From each result above, analyze the `P>|t|` column in the summary above.\n",
    "\n",
    "- **Hypothesis**: Null Hypothesis ($H_0$) is that the coefficient is 0 (no effect). If $P < 0.05$, we reject $H_0$.\n",
    "\n",
    "- **Observation**: Look for features with P-values $> 0.05$.\n",
    "\n",
    "- **Refinement**: Below, we will try dropping a feature if it appears insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463daff8",
   "metadata": {},
   "source": [
    "\n",
    "## **2\\. Polynomial Features (Complexity)**\n",
    "\n",
    "Simple linear models assume a straight-line relationship. However, biological growth is often non-linear. We will introduce **Polynomial Features** to capture interaction terms (e.g., $Length \\times Diameter$) and curvature.\n",
    "\n",
    "**A. Creating Interaction Terms**\n",
    "Use `PolynomialFeatures` to generate a new feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa99d00",
   "metadata": {},
   "source": [
    "\n",
    "**B. The Danger of Overfitting**\n",
    "Fit a standard Linear Regression model on these new polynomial features. Notice the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89776fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67200086",
   "metadata": {},
   "source": [
    "\n",
    "## **3\\. Regularization (Ridge & Lasso)**\n",
    "\n",
    "High-degree polynomials can lead to exploding coefficients. One may use **Regularization** to constrain the model, trading a little bias for significantly reduced variance.\n",
    "\n",
    "**A. Ridge Regression (L2 Penalty)** \n",
    "\n",
    "You may use a `Pipeline` to ensure data is Scaled before Regularization (crucial for Ridge/Lasso). We use `RidgeCV` to automatically find the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b62f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43545b7",
   "metadata": {},
   "source": [
    "\n",
    "**B. Visualizing the Regularization Path (Ridge)**\n",
    "\n",
    "This plot visualizes how increasing the regularization strength ($\\alpha$) shrinks the coefficients towards zero, reducing model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328890b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb52972",
   "metadata": {},
   "source": [
    "\n",
    "**C. Lasso feature selection**\n",
    "\n",
    "Unlike Ridge, Lasso can drive coefficients exactly to zero, effectively performing feature selection. Build polynomial of a chosen degree of selected features, then perform lasso regression. Fine-tune suitable penalization strength $\\alpha$ and keep track the change in coefficients of the model at each value of $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb976bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac39ee",
   "metadata": {},
   "source": [
    "- Interpret each of your obtained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cced85e",
   "metadata": {},
   "source": [
    "## **References & Further Reading**\n",
    "\n",
    "* **Textbook**: [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar.\n",
    "    * *Recommended Reading*: Chapter 4.1 on [Linear Regression](https://christophm.github.io/interpretable-ml-book/limo.html) for a deeper understanding of weights, p-values, and interpretation.\n",
    "\n",
    "* **Documentation**:\n",
    "    * [Statsmodels OLS](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html): Official documentation for the Ordinary Least Squares implementation used for statistical testing.\n",
    "    * [Scikit-Learn: Polynomial Features](https://scikit-learn.org/stable/modules/preprocessing.html#polynomial-features): Guide on generating polynomial and interaction features to capture non-linear relationships.\n",
    "    * [Scikit-Learn: Generalized Linear Models](https://scikit-learn.org/stable/modules/linear_model.html): Comprehensive documentation for Ridge (L2) and Lasso (L1) regularization methods.\n",
    "\n",
    "* **Dataset**:\n",
    "    * [UCI Machine Learning Repository: Abalone Dataset](https://archive.ics.uci.edu/ml/datasets/abalone): The original source of the data, including variable definitions and data collection details.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
